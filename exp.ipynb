{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import arcpy\n",
    "import re \n",
    "\n",
    "\n",
    "# Data from https://feederwatch.org/explore/raw-dataset-requests/\n",
    "\n",
    "def get_species_codes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function queries the Species Codes sheet from the FeederWatch Data Dictionary. The\n",
    "    data is available through an excel sheet provided in the data website. This data will be \n",
    "    used to access the corresponding names and families of the different species codes.\n",
    "    Returns a pandas dataframe of species (Fields: species_code, species_name, family)\n",
    "    \"\"\"\n",
    "    # First, set up the url for the data dictionary (Google Drive).\n",
    "    # Credit goes to the following StackOverflow answer for re-formatting the url:\n",
    "    # https://stackoverflow.com/questions/56611698/pandas-how-to-read-csv-file-from-google-drive-public\n",
    "    url = 'https://drive.google.com/file/d/1kHmx2XhA2MJtEyTNMpwqTQEnoa9M7Il2/view?usp=sharing'\n",
    "    url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "    # Read the Excel Sheet with the Species Codes\n",
    "    species = pd.read_excel(url, sheet_name='Species Codes', header=1)\n",
    "    # Filter and rename columns\n",
    "    species = species[['SPECIES_CODE', 'PRI_COM_NAME_INDXD', 'FAMILY']]\\\n",
    "        .rename(columns={'SPECIES_CODE':'species_code', \n",
    "                         'PRI_COM_NAME_INDXD':'species_name',\n",
    "                         'FAMILY':'family'})\n",
    "    return species\n",
    "\n",
    "def clean_fw_data(data:pd.DataFrame, \n",
    "                  birds:pd.DataFrame, \n",
    "                  sub_national_code:list=[]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function cleans the FeederWatch data so that it only contains \n",
    "    relevent fields, accurate (valid) data, specified birds, and specified locations.\n",
    "    Args:\n",
    "    - data: a pandas dataframe; raw data downloaded from FeederWatch site\n",
    "    - birds: species data - output of get_species_codes(). \n",
    "             * Note: It can be a subset of this data (e.g., a specific family)\n",
    "    - sub_national_code: list of `subnational1_code` fields to filter to \n",
    "    Returns a subset of the original data input, with cleaned field names.\n",
    "    \"\"\"\n",
    "    # All available names of fields in dataset\n",
    "    all_names = ['loc_id', 'latitude', 'longitude', 'subnational1_code', \n",
    "                 'entry_technique', 'sub_id', 'obs_id', 'month', 'day',\n",
    "                 'year', 'proj_period_id', 'species_code', 'how_many',\n",
    "                 'valid', 'reviewed', 'plus_code', 'day1_am', 'day1_pm',\n",
    "                 'day2_am', 'day2_pm', 'effort_hrs_atleast', \n",
    "                 'snow_dep_atleast', 'data_entry_method']\n",
    "    # Output names of fields in dataset (to be kept)\n",
    "    out_names = ['species_code', 'species_name', 'how_many', 'loc_id', 'latitude', 'longitude', 'subnational1_code',\n",
    "                 'date', 'observation_period', 'day1_am', 'day1_pm', \n",
    "                 'day2_am', 'day2_pm']\n",
    "    # Preprocessing (fix column names, include/exclude fields)\n",
    "    data.rename(columns=str.lower, inplace=True)\n",
    "    other_names = [n for n in all_names if n not in data.columns]\n",
    "    data = data.assign(**{name:np.nan for name in other_names if len(other_names) > 0})\n",
    "    # Filter Data by valid, no plus_code, species, optional location\n",
    "    data = data.query(f'valid == 1 & plus_code != 1 & species_code == @birds.species_code.to_list()')\n",
    "    if sub_national_code is not None:\n",
    "        data = data.loc[data.subnational1_code.isin(sub_national_code)]\n",
    "    # Join with species (to get species name)\n",
    "    data = pd.merge(data, birds, how='left', on='species_code')\n",
    "    # Date formatting\n",
    "    data['date'] = pd.to_datetime(dict(year=data.year, \n",
    "                                       month=data.month, \n",
    "                                       day=data.day))\n",
    "    data['observation_period'] = data.date.astype(str) + \" to \" + (data.date + pd.Timedelta(days=1)).astype(str)\n",
    "    # Return, Ensuring correct order, specific output columns, sorted\n",
    "    return data[out_names].sort_values(by=['date', 'species_name'], ascending=[True, True])\n",
    "\n",
    "def getFeedWatcherData(outfile:str,\n",
    "                       tfs:list, \n",
    "                       birds:pd.DataFrame, \n",
    "                       sub_national_code:list=[], \n",
    "                       out_dir:str='data', \n",
    "                       file_suffix:str='',\n",
    "                       save_:bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets FeederWatch data from website. When reading directly from the URLs \n",
    "    and saving the output, this can take a while (depending on internet speed).\n",
    "    Each independent query (by date range) is saved to a gzipped .csv file,\n",
    "    so if the process is interrupted or re-run, it can be read directly from\n",
    "    that file instead of re-downloaded. Data is also cleaned/filtered (using \n",
    "    `clean_fw_data()`), then concatenated and saved to a final .csv file.\n",
    "    Args: \n",
    "    - outfile: Final output file name\n",
    "    - tfs: Time-frames to get data for\n",
    "    - birds: Species data (Optionally) pre-filtered (e.g., by family)\n",
    "    - sub_national_code: (Optionally) filter by subnational1_code (e.g., U.S. State)\n",
    "    - out_dir: Directory in which to save data\n",
    "    - file_suffix: Suffix of file names\n",
    "    - save_: Whether or not to save the output to a gzipped file\n",
    "    Returns a pandas dataframe of the selected FeederWatch bird data\n",
    "    \"\"\"\n",
    "    final_out_file = os.path.join(out_dir, outfile)\n",
    "    # First check if the file already exists\n",
    "    if os.path.isfile(final_out_file):\n",
    "        out = pd.read_csv(final_out_file)\n",
    "        out['date'] = pd.to_datetime(out.date)\n",
    "    else:\n",
    "        df_lis = list()\n",
    "        for i in np.arange(0, len(tfs)):\n",
    "            # Read Data (either from URL, or from previously saved data if available)\n",
    "            tf = tfs[i]\n",
    "            out_file = os.path.join(out_dir, f'FW_{tf}_{file_suffix}.csv.gz')\n",
    "            if not os.path.isfile(out_file):\n",
    "                url = 'https://clo-pfw-prod.s3.us-west-2.amazonaws.com/data/PFW_' + tf + '_public.csv'\n",
    "                print(f\"Getting {tf} data from {url}\")\n",
    "                # Read/Clean data\n",
    "                data = clean_fw_data(data=pd.read_csv(url), \n",
    "                                    birds=birds, \n",
    "                                    sub_national_code=sub_national_code)\n",
    "                if save_:\n",
    "                    # If not previously cached, save as gzip\n",
    "                    print(f\"Saving {tf} data to {out_file}\")\n",
    "                    data.to_csv(out_file, compression='gzip', index=False)\n",
    "            else:\n",
    "                print(f\"Reading {tf} data from {out_file}\")\n",
    "                data = pd.read_csv(out_file, compression='gzip')\n",
    "            # Append to list\n",
    "            df_lis.append(data)\n",
    "        # Combine list into single dataframe\n",
    "        print(\"Concatenating list of dataframes\")\n",
    "        out = pd.concat(df_lis)\n",
    "        # Save to file\n",
    "        out.to_csv(final_out_file, index=False)\n",
    "    return out\n",
    "\n",
    "# Timeframes available in FeederWatch\n",
    "DATA_TIMEFRAMES = ['1988_1995', '1996_2000', '2001_2005', \n",
    "                   '2006_2010', '2011_2015', '2016_2020', \n",
    "                   '2021']\n",
    "# All Species\n",
    "SPECIES = get_species_codes()\n",
    "# Woodpecker Family\n",
    "WOODPECKERS = SPECIES.loc[SPECIES['family'] == 'Picidae (Woodpeckers)']\n",
    "\n",
    "fw = getFeedWatcherData(outfile=\"FW_woodpeckers_NC.csv\",\n",
    "                        tfs=DATA_TIMEFRAMES,\n",
    "                        birds=WOODPECKERS,\n",
    "                        sub_national_code=['US-NC'],\n",
    "                        out_dir='data',\n",
    "                        file_suffix='woodpeckers_NC',\n",
    "                        save_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species():\n",
    "    def __init__(self, dataframe:pd.DataFrame) -> None:\n",
    "        self.species_code = dataframe.species_code.to_list()\n",
    "        self.species_name = dataframe.species_name.to_list()\n",
    "        self.family = dataframe.family.to_list()\n",
    "\n",
    "class Bird(Species):\n",
    "    def __init__(self, dataframe:pd.DataFrame, bird_name:str) -> None:\n",
    "        super().__init__(dataframe)\n",
    "        # Get index from original dataframe\n",
    "        bird_idx = np.array([bird_name == b for b in self.species_name])\n",
    "        # Create attributes\n",
    "        self.code = str(np.array(self.species_code)[bird_idx][0])\n",
    "        self.name = str(np.array(self.species_name)[bird_idx][0])\n",
    "        self.family = str(np.array(self.family)[bird_idx][0])\n",
    "        # Adjust name for formatted feature class name attribute\n",
    "        name_parts = self.name.split(', ')\n",
    "        self.formatted_name = re.sub(\"[()]\", \"\", name_parts[1] + \"_\" + name_parts[0])\\\n",
    "                .replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        self.fc_name = f\"FW_{self.formatted_name}_NC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Globals; Setup\n",
    "PROJ_PATH = \"C:/Users/bento/OneDrive/code_and_data/ncsu-mgist/courses/gis_540/final_project\"\n",
    "DB_PATH = \"fw_GDB.gdb\"\n",
    "# Create File Geodatabase\n",
    "if not os.path.exists(os.path.join(PROJ_PATH, DB_PATH)):\n",
    "    arcpy.CreateFileGDB_management(PROJ_PATH, DB_PATH)\n",
    "arcpy.env.workspace = os.path.join(PROJ_PATH, DB_PATH)\n",
    "DATA_PATH = os.path.join(PROJ_PATH, \"data\")\n",
    "FW_FILE = \"FW_woodpeckers_NC.csv\"\n",
    "BASE_FC = \"FW_woodpeckers_NC\"\n",
    "EXISTING_FCS = arcpy.ListFeatureClasses()\n",
    "# Projected Coordinate System\n",
    "COORD_SYSTEM = arcpy.SpatialReference(\"NAD 1983 StatePlane North Carolina FIPS 3200 (US Feet)\")\n",
    "\n",
    "def batchBirdAnalysis(fw_file:str, \n",
    "                      base_fc:str,\n",
    "                      existing_fcs:list,\n",
    "                      out_coordinate_system:arcpy.SpatialReference, \n",
    "                      data_path:str,\n",
    "                      fw_df:pd.DataFrame,\n",
    "                      species_df:pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Batch processing and analysis of FeederWatch bird data. \n",
    "    Steps:\n",
    "    1) Creates Feature Class from .csv file in file Geodatabase\n",
    "    2) Adds projection, saving to new Feature Class\n",
    "    3) Filters Projected Feature Class by species, saving individual\n",
    "       species to their own Feature Classes in the database\n",
    "    4) Creates Space-Time-Cube for each species. Note that this might\n",
    "       fail if there is insufficient data (< 60 records). If this is\n",
    "       the case, the remaining steps will be skipped for this species\n",
    "    5) Creates 3D Visualizaton\n",
    "    6) Identifies Outliers\n",
    "    7) Computes time series clustering analysis\n",
    "    Args: \n",
    "    - fw_file: FeederWatch data .csv file\n",
    "    - base_fc: Base Feature Class name\n",
    "    - existing_fcs: List of existing Feature Classes already saved to the \n",
    "      database (if they already exist, they will be skipped during batch\n",
    "      processing)\n",
    "    - out_coordinate_system: Projected coordinate system\n",
    "    - data_path: Path to save space-time-cubes\n",
    "    - fw_df: FeederWatch dataframe\n",
    "    - species_df: Species dataframe\n",
    "    \"\"\"\n",
    "    # Create base feature class\n",
    "    if base_fc not in existing_fcs:\n",
    "        arcpy.management.XYTableToPoint(in_table=os.path.join(data_path, fw_file),\n",
    "                                        out_feature_class=base_fc,\n",
    "                                        x_field=\"longitude\", \n",
    "                                        y_field=\"latitude\")\n",
    "    # Add projection to base FC\n",
    "    if f\"{base_fc}_projected\" not in existing_fcs:\n",
    "        arcpy.management.Project(base_fc, \n",
    "                                f\"{base_fc}_projected\", \n",
    "                                out_coordinate_system)\n",
    "    # Add to GDB by species\n",
    "    for species_name in fw_df.species_name.unique():\n",
    "        brd = Bird(dataframe=species_df, bird_name=species_name)\n",
    "        if brd.fc_name not in existing_fcs:\n",
    "            print(f'Adding {brd.name} to gdb...')\n",
    "            arcpy.analysis.Select(f\"{base_fc}_projected\", \n",
    "                                brd.fc_name, \n",
    "                                f\"species_name = '{brd.name}'\")\n",
    "        # Create Space-Time-Cube (.nc file)\n",
    "        if f\"{brd.fc_name}.nc\" not in os.listdir(data_path):\n",
    "            try:\n",
    "            # https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/create-space-time-cube.htm\n",
    "                arcpy.stpm.CreateSpaceTimeCube(brd.fc_name, \n",
    "                                            os.path.join(data_path, brd.fc_name),\n",
    "                                            time_field=\"date\", \n",
    "                                            time_step_interval=\"1 Months\", \n",
    "                                            time_step_alignment=\"START_TIME\", \n",
    "                                            distance_interval=\"3 Miles\", \n",
    "                                            summary_fields=\"how_many MEAN ZEROS\", \n",
    "                                            aggregation_shape_type=\"HEXAGON_GRID\") \n",
    "                print(f'Created Space-Time Cube for {brd.name} in data folder.\\n')\n",
    "            except arcpy.ExecuteError:\n",
    "                print(arcpy.GetMessages())\n",
    "                print(f\"Skipping {brd.name}...\\n\")\n",
    "        \n",
    "        if f\"{brd.fc_name}.nc\" in os.listdir(data_path):\n",
    "            # Create 3D Visualization\n",
    "            if f\"{brd.fc_name}_Visualize3D\" not in existing_fcs:\n",
    "                arcpy.stpm.VisualizeSpaceTimeCube3D(os.path.join(data_path, f\"{brd.fc_name}.nc\"), \n",
    "                                        \"HOW_MANY_MEAN_ZEROS\", \n",
    "                                        \"VALUE\", \n",
    "                                        f\"{brd.fc_name}_Visualize3D\")\n",
    "                print(f\"Created 3D Visualization for {brd.name}.\\n\")\n",
    "            # Find outliers\n",
    "            if f\"{brd.fc_name}_outliers\" not in existing_fcs:\n",
    "                try:\n",
    "                    # https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/localoutlieranalysis.htm\n",
    "                    print(f'Finding outliers for {brd.name}...\\n')\n",
    "                    arcpy.stpm.LocalOutlierAnalysis(in_cube=os.path.join(data_path, f\"{brd.fc_name}.nc\"), \n",
    "                                                    analysis_variable=\"HOW_MANY_MEAN_ZEROS\", \n",
    "                                                    output_features=f\"{brd.fc_name}_outliers\", \n",
    "                                                    neighborhood_distance=\"25 Miles\", \n",
    "                                                    neighborhood_time_step=3, \n",
    "                                                    number_of_permutations=499, \n",
    "                                                    conceptualization_of_spatial_relationships=\"FIXED_DISTANCE\")\n",
    "                except arcpy.ExecuteError:\n",
    "                    print(arcpy.GetMessages())\n",
    "                    print(f\"Skipping {brd.name}...\\n\")\n",
    "            # Time Series Clustering Analysis\n",
    "            if f\"{brd.fc_name}_ts_clusters\" not in existing_fcs:\n",
    "                try:\n",
    "                    print(f'Time Series Clustering for {brd.name}...\\n')\n",
    "                    # https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/time-series-clustering.htm\n",
    "                    arcpy.stpm.TimeSeriesClustering(in_cube=os.path.join(data_path, f\"{brd.fc_name}.nc\"), \n",
    "                                                    analysis_variable=\"HOW_MANY_MEAN_ZEROS\", \n",
    "                                                    output_features=f\"{brd.fc_name}_ts_clusters\", \n",
    "                                                    characteristic_of_interest=\"PROFILE_FOURIER\", \n",
    "                                                    cluster_count=3, \n",
    "                                                    output_table_for_charts=f\"{brd.fc_name}_ts_clusters_table\", \n",
    "                                                    shape_characteristic_to_ignore=\"TIME_LAG\", \n",
    "                                                    enable_time_series_popups=\"CREATE_POPUP\")\n",
    "                except arcpy.ExecuteError:\n",
    "                    print(arcpy.GetMessages())\n",
    "                    print(f\"Skipping {brd.name}...\\n\")\n",
    "        \n",
    "batchBirdAnalysis(fw_file=FW_FILE, \n",
    "                  base_fc=BASE_FC,\n",
    "                  existing_fcs=EXISTING_FCS, \n",
    "                  out_coordinate_system=COORD_SYSTEM,\n",
    "                  data_path=DATA_PATH,\n",
    "                  fw_df=fw,\n",
    "                  species_df=WOODPECKERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f192acd1de90387fc8829f885aaa95d2215ac7117848e2dc5e90bd1eb94ec849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
